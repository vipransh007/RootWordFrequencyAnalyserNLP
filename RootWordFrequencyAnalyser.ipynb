{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14fd1001-9db0-472a-9a0e-58363f08c614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4cf56e1-085b-4a1f-b4d8-7b625d8d3568",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- NLTK Resource Download ---\n",
    "# The following lines download the necessary NLTK data.\n",
    "# You only need to run this once per environment. If you have them downloaded,\n",
    "# you can comment these lines out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dcebcc-412a-4764-b7de-9015ab49907d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please paste the text you want to analyze below and press Enter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " They are considered different base words by the dictionary. While conceptually related, forcing the script to change a noun into its adjective form is outside the scope of standard lemmatization and could cause other words to be converted incorrectly. For this project, sticking to the dictionary root is the most reliable approach.\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\vipra/nltk_data'\n    - 'C:\\\\Users\\\\vipra\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\vipra\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\vipra\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\vipra\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m user_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m()\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Get the frequency of root words from the user's text\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m word_frequencies \u001b[38;5;241m=\u001b[39m get_root_word_frequencies(user_text)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# --- Display Results ---\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Root Word Frequencies ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 36\u001b[0m, in \u001b[0;36mget_root_word_frequencies\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     33\u001b[0m lower_text \u001b[38;5;241m=\u001b[39m cleaned_text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# 2. Tokenization\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(lower_text)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# 3. Part-of-Speech (POS) Tagging\u001b[39;00m\n\u001b[0;32m     39\u001b[0m pos_tagged_tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mpos_tag(tokens)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, path \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\vipra/nltk_data'\n    - 'C:\\\\Users\\\\vipra\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\vipra\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\vipra\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\vipra\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Helper Function for Part-of-Speech (POS) Tagging ---\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Maps treebank POS tags to WordNet POS tags.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # By default, return NOUN if the tag is not recognized\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# --- Main Function to Process Text ---\n",
    "def get_root_word_frequencies(text):\n",
    "\n",
    "    # 1. Clean and Normalize Text\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    lower_text = cleaned_text.lower()\n",
    "\n",
    "    # 2. Tokenization\n",
    "    tokens = word_tokenize(lower_text)\n",
    "\n",
    "    # 3. Part-of-Speech (POS) Tagging\n",
    "    pos_tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "    # 4. Lemmatization (Finding the Root Word)\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    root_words = []\n",
    "    for word, tag in pos_tagged_tokens:\n",
    "        pos = get_wordnet_pos(tag)\n",
    "        root_word = lemmatizer.lemmatize(word, pos=pos)\n",
    "        \n",
    "        # If the word is a noun or adjective and didn't change,\n",
    "        # try lemmatizing it again as a verb. This helps find the\n",
    "        # core concept (e.g., 'interaction' -> 'interact').\n",
    "        if (pos == wordnet.NOUN or pos == wordnet.ADJ) and root_word == word:\n",
    "            root_word = lemmatizer.lemmatize(word, pos=wordnet.VERB)\n",
    "\n",
    "        root_words.append(root_word)\n",
    "        \n",
    "    # 5. Frequency Counting\n",
    "    frequency_counts = Counter(root_words)\n",
    "    \n",
    "    return frequency_counts\n",
    "\n",
    "# --- Example Usage ---\n",
    "# The script will now ask for user input when the cell is run.\n",
    "print(\"Please paste the text you want to analyze below and press Enter.\")\n",
    "user_text = input()\n",
    "\n",
    "# Get the frequency of root words from the user's text\n",
    "word_frequencies = get_root_word_frequencies(user_text)\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"\\n--- Root Word Frequencies ---\")\n",
    "print(f\"Found {len(word_frequencies)} unique root words.\\n\")\n",
    "    \n",
    "# Sort the results by frequency in descending order for better readability\n",
    "sorted_frequencies = word_frequencies.most_common()\n",
    "\n",
    "for word, count in sorted_frequencies:\n",
    "    print(f\"- '{word}': {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab0ee18d-53e7-4120-bbef-aa1e509c4f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NLTK resources...\n",
      "NLTK resources are ready.\n",
      "\n",
      "Please paste the text you want to analyze below and press Enter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The unstructured nature of the scene required a complete reconstruction of the actor's process; his actions and reactions were critical for the construction of a believable interaction, but his tendency for overacting ultimately led to the scene's destruction.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Root Word Frequencies ---\n",
      "Found 28 unique root words.\n",
      "\n",
      "- 'the': 5\n",
      "- 'of': 3\n",
      "- 'scene': 2\n",
      "- 'a': 2\n",
      "- 'his': 2\n",
      "- 'for': 2\n",
      "- 'unstructured': 1\n",
      "- 'nature': 1\n",
      "- 'require': 1\n",
      "- 'complete': 1\n",
      "- 'reconstruction': 1\n",
      "- 'actor': 1\n",
      "- 'process': 1\n",
      "- 'action': 1\n",
      "- 'and': 1\n",
      "- 'reaction': 1\n",
      "- 'be': 1\n",
      "- 'critical': 1\n",
      "- 'construction': 1\n",
      "- 'believable': 1\n",
      "- 'interaction': 1\n",
      "- 'but': 1\n",
      "- 'tendency': 1\n",
      "- 'overact': 1\n",
      "- 'ultimately': 1\n",
      "- 'lead': 1\n",
      "- 'to': 1\n",
      "- 'destruction': 1\n"
     ]
    }
   ],
   "source": [
    "# It's a good practice to import all libraries at the top of the cell\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# --- NLTK Resource Download ---\n",
    "# This is a more robust way to ensure the necessary NLTK data is downloaded.\n",
    "# It will open a downloader window if the packages are not found.\n",
    "# If you are in a non-graphical environment, this might hang.\n",
    "# In that case, run these lines in a separate Python script first.\n",
    "print(\"Checking for NLTK resources...\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "print(\"NLTK resources are ready.\")\n",
    "\n",
    "\n",
    "# --- Helper Function for Part-of-Speech (POS) Tagging ---\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Maps treebank POS tags to WordNet POS tags.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # By default, return NOUN if the tag is not recognized\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# --- Main Function to Process Text ---\n",
    "def get_root_word_frequencies(text):\n",
    "\n",
    "    # 1. Clean and Normalize Text\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    lower_text = cleaned_text.lower()\n",
    "\n",
    "    # 2. Tokenization\n",
    "    tokens = word_tokenize(lower_text)\n",
    "\n",
    "    # 3. Part-of-Speech (POS) Tagging\n",
    "    pos_tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "    # 4. Lemmatization (Finding the Root Word)\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    root_words = []\n",
    "    for word, tag in pos_tagged_tokens:\n",
    "        pos = get_wordnet_pos(tag)\n",
    "        root_word = lemmatizer.lemmatize(word, pos=pos)\n",
    "        \n",
    "        # If the word is a noun or adjective and didn't change,\n",
    "        # try lemmatizing it again as a verb. This helps find the\n",
    "        # core concept (e.g., 'interaction' -> 'interact').\n",
    "        if (pos == wordnet.NOUN or pos == wordnet.ADJ) and root_word == word:\n",
    "            root_word = lemmatizer.lemmatize(word, pos=wordnet.VERB)\n",
    "\n",
    "        root_words.append(root_word)\n",
    "        \n",
    "    # 5. Frequency Counting\n",
    "    frequency_counts = Counter(root_words)\n",
    "    \n",
    "    return frequency_counts\n",
    "\n",
    "# --- Example Usage ---\n",
    "# The script will now ask for user input when the cell is run.\n",
    "print(\"\\nPlease paste the text you want to analyze below and press Enter.\")\n",
    "user_text = input()\n",
    "\n",
    "# Get the frequency of root words from the user's text\n",
    "word_frequencies = get_root_word_frequencies(user_text)\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"\\n--- Root Word Frequencies ---\")\n",
    "print(f\"Found {len(word_frequencies)} unique root words.\\n\")\n",
    "    \n",
    "# Sort the results by frequency in descending order for better readability\n",
    "sorted_frequencies = word_frequencies.most_common()\n",
    "\n",
    "for word, count in sorted_frequencies:\n",
    "\n",
    "    print(f\"- '{word}': {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3320c8a-223d-4cbf-b3be-d7037be77ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NLTK resources...\n",
      "NLTK resources are ready.\n",
      "\n",
      "Please paste the text you want to analyze below and press Enter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The disagreeable hater's deep-seated hatred was not well understood by the loving, lovable child, who only loves everyone. For true understanding, one must have understood the core disagreement, as simply hating is not an agreeable solution. The final compilation required a complete recompilation of all previously compiled modules\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stemmed Word Frequencies ---\n",
      "Found 41 unique word stems.\n",
      "\n",
      "- 'the': 4\n",
      "- 'not': 2\n",
      "- 'understood': 2\n",
      "- 'love': 2\n",
      "- 'compil': 2\n",
      "- 'disagre': 1\n",
      "- 'hater': 1\n",
      "- 'deepseat': 1\n",
      "- 'hatr': 1\n",
      "- 'wa': 1\n",
      "- 'well': 1\n",
      "- 'by': 1\n",
      "- 'lovabl': 1\n",
      "- 'child': 1\n",
      "- 'who': 1\n",
      "- 'onli': 1\n",
      "- 'everyon': 1\n",
      "- 'for': 1\n",
      "- 'true': 1\n",
      "- 'understand': 1\n",
      "- 'one': 1\n",
      "- 'must': 1\n",
      "- 'have': 1\n",
      "- 'core': 1\n",
      "- 'disagr': 1\n",
      "- 'as': 1\n",
      "- 'simpli': 1\n",
      "- 'hate': 1\n",
      "- 'is': 1\n",
      "- 'an': 1\n",
      "- 'agreeabl': 1\n",
      "- 'solut': 1\n",
      "- 'final': 1\n",
      "- 'requir': 1\n",
      "- 'a': 1\n",
      "- 'complet': 1\n",
      "- 'recompil': 1\n",
      "- 'of': 1\n",
      "- 'all': 1\n",
      "- 'previous': 1\n",
      "- 'modul': 1\n"
     ]
    }
   ],
   "source": [
    "# It's a good practice to import all libraries at the top of the cell\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer # Importing the Porter Stemmer\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# --- NLTK Resource Download ---\n",
    "# This ensures the 'punkt' tokenizer is available.\n",
    "print(\"Checking for NLTK resources...\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "print(\"NLTK resources are ready.\")\n",
    "\n",
    "\n",
    "# --- Main Function to Process Text using Stemming ---\n",
    "def get_stemmed_word_frequencies(text):\n",
    "\n",
    "\n",
    "    # 1. Clean and Normalize Text\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    lower_text = cleaned_text.lower()\n",
    "\n",
    "    # 2. Tokenization\n",
    "    tokens = word_tokenize(lower_text)\n",
    "\n",
    "    # 3. Stemming (Finding the Root Stem)\n",
    "    # This is more aggressive than lemmatization and will chop off\n",
    "    # prefixes and suffixes.\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = []\n",
    "    for word in tokens:\n",
    "        stem = stemmer.stem(word)\n",
    "        stemmed_words.append(stem)\n",
    "        \n",
    "    # 4. Frequency Counting\n",
    "    frequency_counts = Counter(stemmed_words)\n",
    "    \n",
    "    return frequency_counts\n",
    "\n",
    "# --- Example Usage ---\n",
    "# The script will now ask for user input when the cell is run.\n",
    "print(\"\\nPlease paste the text you want to analyze below and press Enter.\")\n",
    "user_text = input()\n",
    "\n",
    "# Get the frequency of root words from the user's text\n",
    "word_frequencies = get_stemmed_word_frequencies(user_text)\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"\\n--- Stemmed Word Frequencies ---\")\n",
    "print(f\"Found {len(word_frequencies)} unique word stems.\\n\")\n",
    "    \n",
    "# Sort the results by frequency in descending order for better readability\n",
    "sorted_frequencies = word_frequencies.most_common()\n",
    "\n",
    "for word, count in sorted_frequencies:\n",
    "    print(f\"- '{word}': {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f65d56-d5cb-4d96-8aed-fd5ce3073056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NLTK resources...\n",
      "NLTK resources are ready.\n",
      "\n",
      "Please paste the text you want to analyze below and press Enter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You have an incredibly sharp eye! Your observations are spot on, and they highlight the most important characteristic of stemming.  You are correct:  unstructured became unstructur  believable became believ  tendency became tendenc  ultimately became ultim  This is not an error in the code, but rather the exact, expected behavior of the Porter Stemmer algorithm we are using.  Here's why:  A stemmer's only goal is to aggressively chop down words so that related words end up with the exact same stem. It follows a strict set of rules and doesn't care if the final stem is a real dictionary word.  It ensures that believes, believing, and believable all get chopped down to the same root: believ.  It ensures that tendency and tendencies both get chopped down to tendenc.  For the purpose of counting word frequencies, this works perfectly because it groups all variations under one common identifier, even if that identifier looks a bit strange to us.  So, while the output might look \"wrong\" from a human perspective, it's \"correct\" for the algorithm's goal of grouping words. You've successfully discovered the fundamental trade-off between Stemming (fast, aggressive, not always readable) and Lemmatization (slower, more precise, always a real word).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stemmed Word Frequencies ---\n",
      "Found 113 unique word stems.\n",
      "\n",
      "- 'the': 11\n",
      "- 'believ': 6\n",
      "- 'a': 6\n",
      "- 'word': 6\n",
      "- 'and': 5\n",
      "- 'of': 5\n",
      "- 'tendenc': 5\n",
      "- 'it': 5\n",
      "- 'stem': 4\n",
      "- 'becam': 4\n",
      "- 'to': 4\n",
      "- 'that': 4\n",
      "- 'are': 3\n",
      "- 'is': 3\n",
      "- 'chop': 3\n",
      "- 'down': 3\n",
      "- 'you': 2\n",
      "- 'an': 2\n",
      "- 'correct': 2\n",
      "- 'unstructur': 2\n",
      "- 'ultim': 2\n",
      "- 'thi': 2\n",
      "- 'not': 2\n",
      "- 'exact': 2\n",
      "- 'stemmer': 2\n",
      "- 'algorithm': 2\n",
      "- 'goal': 2\n",
      "- 'aggress': 2\n",
      "- 'so': 2\n",
      "- 'same': 2\n",
      "- 'if': 2\n",
      "- 'real': 2\n",
      "- 'ensur': 2\n",
      "- 'all': 2\n",
      "- 'get': 2\n",
      "- 'for': 2\n",
      "- 'group': 2\n",
      "- 'identifi': 2\n",
      "- 'look': 2\n",
      "- 'alway': 2\n",
      "- 'have': 1\n",
      "- 'incred': 1\n",
      "- 'sharp': 1\n",
      "- 'eye': 1\n",
      "- 'your': 1\n",
      "- 'observ': 1\n",
      "- 'spot': 1\n",
      "- 'on': 1\n",
      "- 'they': 1\n",
      "- 'highlight': 1\n",
      "- 'most': 1\n",
      "- 'import': 1\n",
      "- 'characterist': 1\n",
      "- 'error': 1\n",
      "- 'in': 1\n",
      "- 'code': 1\n",
      "- 'but': 1\n",
      "- 'rather': 1\n",
      "- 'expect': 1\n",
      "- 'behavior': 1\n",
      "- 'porter': 1\n",
      "- 'we': 1\n",
      "- 'use': 1\n",
      "- 'here': 1\n",
      "- 'whi': 1\n",
      "- 'onli': 1\n",
      "- 'relat': 1\n",
      "- 'end': 1\n",
      "- 'up': 1\n",
      "- 'with': 1\n",
      "- 'follow': 1\n",
      "- 'strict': 1\n",
      "- 'set': 1\n",
      "- 'rule': 1\n",
      "- 'doesnt': 1\n",
      "- 'care': 1\n",
      "- 'final': 1\n",
      "- 'dictionari': 1\n",
      "- 'root': 1\n",
      "- 'both': 1\n",
      "- 'purpos': 1\n",
      "- 'count': 1\n",
      "- 'frequenc': 1\n",
      "- 'work': 1\n",
      "- 'perfectli': 1\n",
      "- 'becaus': 1\n",
      "- 'variat': 1\n",
      "- 'under': 1\n",
      "- 'one': 1\n",
      "- 'common': 1\n",
      "- 'even': 1\n",
      "- 'bit': 1\n",
      "- 'strang': 1\n",
      "- 'us': 1\n",
      "- 'while': 1\n",
      "- 'output': 1\n",
      "- 'might': 1\n",
      "- 'wrong': 1\n",
      "- 'from': 1\n",
      "- 'human': 1\n",
      "- 'perspect': 1\n",
      "- 'youv': 1\n",
      "- 'success': 1\n",
      "- 'discov': 1\n",
      "- 'fundament': 1\n",
      "- 'tradeoff': 1\n",
      "- 'between': 1\n",
      "- 'fast': 1\n",
      "- 'readabl': 1\n",
      "- 'lemmat': 1\n",
      "- 'slower': 1\n",
      "- 'more': 1\n",
      "- 'precis': 1\n"
     ]
    }
   ],
   "source": [
    "# It's a good practice to import all libraries at the top of the cell\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer # Importing the Porter Stemmer\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# --- NLTK Resource Download ---\n",
    "# This ensures the 'punkt' tokenizer is available.\n",
    "print(\"Checking for NLTK resources...\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "print(\"NLTK resources are ready.\")\n",
    "\n",
    "\n",
    "# --- Main Function to Process Text using Stemming ---\n",
    "def get_stemmed_word_frequencies(text):\n",
    "\n",
    "    # 1. Clean and Normalize Text\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    lower_text = cleaned_text.lower()\n",
    "\n",
    "    # 2. Tokenization\n",
    "    tokens = word_tokenize(lower_text)\n",
    "\n",
    "    # 3. Stemming (Finding the Root Stem)\n",
    "    # This is more aggressive than lemmatization and will chop off\n",
    "    # prefixes and suffixes.\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = []\n",
    "    for word in tokens:\n",
    "        stem = stemmer.stem(word)\n",
    "        stemmed_words.append(stem)\n",
    "        \n",
    "    # 4. Frequency Counting\n",
    "    frequency_counts = Counter(stemmed_words)\n",
    "    \n",
    "    return frequency_counts\n",
    "\n",
    "# --- Example Usage ---\n",
    "# The script will now ask for user input when the cell is run.\n",
    "print(\"\\nPlease paste the text you want to analyze below and press Enter.\")\n",
    "user_text = input()\n",
    "\n",
    "# Get the frequency of root words from the user's text\n",
    "word_frequencies = get_stemmed_word_frequencies(user_text)\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"\\n--- Stemmed Word Frequencies ---\")\n",
    "print(f\"Found {len(word_frequencies)} unique word stems.\\n\")\n",
    "    \n",
    "# Sort the results by frequency in descending order for better readability\n",
    "sorted_frequencies = word_frequencies.most_common()\n",
    "\n",
    "for word, count in sorted_frequencies:\n",
    "    print(f\"- '{word}': {count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
